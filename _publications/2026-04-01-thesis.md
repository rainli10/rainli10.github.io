---
title: "Enabling Long-horizon Vision-Language-Navigation with Real-time scene-graph memory module"
collection: publications
category: Ongoing Projects
permalink: /publication/2026-04-01-thesis
excerpt: 'Current vision-language models often lack structural scene understanding and fine-grained details information. These VLNs typically do not maintain an explicit structured memory module to store and organize previously observed information. This limitation becomes especially pronounced in long-horizon tasks. In this work, we propose a novel pipeline that constructs an open-vocabulary，real-time scene-graph from RGBD inputs, video captions and other multimodal inferred information.  Our method further clusters scene-graph objects and regions in real-time. Building on this representation, we introduce an LLM-based planning agent that reasons over the continuously updated scene graph to decompose long-horizon instructions, and automatically detect deviation from goal and perform corrective replanning. The agent then publishes more detailed, short-term sub-instructions to a downstream VLN model instead of original long-horizon instructions. We also developed RL-based Husky Locomotion module for downstream of VLN and plan to test in real world scenarios'
date: 2026-04-01
venue: 'Undergraduate Thesis, expected finished by April and submit to conference'
citation: 'During student researcher at <strong>Toronto Robotics and AI Lab</strong>'
header:
  teaser: "accelerometer_teaser.png"
  teaser_hover: "accelerometer_hover.png"
---
Current vision-language models often lack structural scene understanding and fine-grained details information. These VLNs typically do not maintain an explicit structured memory module to store and organize previously observed information. This limitation becomes especially pronounced in long-horizon tasks. In this work, we propose a novel pipeline that constructs an open-vocabulary，real-time scene-graph from RGBD inputs, video captions and other multimodal inferred information.  Our method further clusters scene-graph objects and regions in real-time. Building on this representation, we introduce an LLM-based planning agent that reasons over the continuously updated scene graph to decompose long-horizon instructions, and automatically detect deviation from goal and perform corrective replanning. The agent then publishes more detailed, short-term sub-instructions to a downstream VLN model instead of original long-horizon instructions. We also developed RL-based Husky Locomotion module for downstream of VLN and plan to test in real world scenarios